{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyf0ikTSZr70yG8AqJrutS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charles980903/Proj2/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZqTjNuIWS0ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146348fa-e64e-45a1-be96-852eadd5f055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please select a download option:\n",
            "1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)\n",
            "2) Kuzushiji-49 (49 classes, 28x28, 270k examples)\n",
            "3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)\n",
            "> 1\n",
            "Please select a download option:\n",
            "1) MNIST data format (ubyte.gz)\n",
            "2) NumPy data format (.npz)\n",
            "> 2\n",
            "Downloading kmnist-train-imgs.npz - 18.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17954/17954 [00:10<00:00, 1698.90KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 211.12KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-imgs.npz - 3.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3008/3008 [00:02<00:00, 1091.28KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 14691.08KB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dataset files downloaded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, total, unit: x  # If tqdm doesn't exist, replace it with a function that does nothing\n",
        "    print('**** Could not import tqdm. Please install tqdm for download progressbars! (pip install tqdm) ****')\n",
        "\n",
        "# Python2 compatibility\n",
        "try:\n",
        "    input = raw_input\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "download_dict = {\n",
        "    '1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)': {\n",
        "        '1) MNIST data format (ubyte.gz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'],\n",
        "        '2) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'],\n",
        "    },\n",
        "    '2) Kuzushiji-49 (49 classes, 28x28, 270k examples)': {\n",
        "        '1) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz',\n",
        "            'http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz'],\n",
        "    },\n",
        "    '3) Kuzushiji-Kanji (3832 classes, 64x64, 140k examples)': {\n",
        "        '1) Folders of images (.tar)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar'],\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "# Download a list of files\n",
        "def download_list(url_list):\n",
        "    for url in url_list:\n",
        "        path = url.split('/')[-1]\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    print('All dataset files downloaded!')\n",
        "\n",
        "# Ask the user about which path to take down the dict\n",
        "def traverse_dict(d):\n",
        "    print('Please select a download option:')\n",
        "    keys = sorted(d.keys())  # Print download options\n",
        "    for key in keys:\n",
        "        print(key)\n",
        "\n",
        "    userinput = input('> ').strip()\n",
        "\n",
        "    try:\n",
        "        selection = int(userinput) - 1\n",
        "    except ValueError:\n",
        "        print('Your selection was not valid')\n",
        "        traverse_dict(d)  # Try again if input was not valid\n",
        "        return\n",
        "\n",
        "    selected = keys[selection]\n",
        "\n",
        "    next_level = d[selected]\n",
        "    if isinstance(next_level, list):  # If we've hit a list of downloads, download that list\n",
        "        download_list(next_level)\n",
        "    else:\n",
        "        traverse_dict(next_level)     # Otherwise, repeat with the next level\n",
        "\n",
        "traverse_dict(download_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Kuzushiji-MNIST data\n",
        "train_images = np.load('kmnist-train-imgs.npz')['arr_0']\n",
        "train_labels = np.load('kmnist-train-labels.npz')['arr_0']\n",
        "test_images = np.load('kmnist-test-imgs.npz')['arr_0']\n",
        "test_labels = np.load('kmnist-test-labels.npz')['arr_0']\n",
        "\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Flatten the images for input into the Stacked Autoencoder\n",
        "train_images_flattened = train_images.reshape(-1, 784)\n",
        "test_images_flattened = test_images.reshape(-1, 784)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_images_flattened, train_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RpHaANG_Xmrn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "VkZB8FQ1gTuZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sae(bottleneck_size=64):\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=(784,))\n",
        "    encoder = Dense(800, activation='relu')(input_layer)\n",
        "    encoder = Dense(200, activation='relu')(encoder)\n",
        "    bottleneck = Dense(bottleneck_size, activation='relu')(encoder)\n",
        "\n",
        "    # Decoder\n",
        "    decoder = Dense(200, activation='relu')(bottleneck)\n",
        "    decoder = Dense(800, activation='relu')(decoder)\n",
        "    output_layer = Dense(784, activation='sigmoid')(decoder)\n",
        "\n",
        "    # Build model\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return autoencoder\n"
      ],
      "metadata": {
        "id": "dJVCALezjuCF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def extract_features(autoencoder, data):\n",
        "\n",
        "    encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[3].output)\n",
        "    features = encoder_model.predict(data)\n",
        "    return features\n",
        "\n",
        "# Define ranges for hyperparameters to test\n",
        "bottleneck_sizes = [32, 64, 128]\n",
        "batch_sizes = [64, 128]\n",
        "patience_values = [5, 10]\n",
        "\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "\n",
        "\n",
        "for bottleneck_size, batch_size, patience in product(bottleneck_sizes, batch_sizes, patience_values):\n",
        "    print(f\"Training SAE with bottleneck size {bottleneck_size}, batch size {batch_size}, patience {patience}\")\n",
        "\n",
        "    # Build the autoencoder\n",
        "    autoencoder = build_sae(bottleneck_size)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "    # Train the autoencoder with early stopping\n",
        "    autoencoder.fit(\n",
        "        x_train, x_train,\n",
        "        epochs=50,  # Set a high epoch limit, early stopping will determine the actual stopping point\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_val, x_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Extract features from the bottleneck layer\n",
        "    train_features = extract_features(autoencoder, x_train)\n",
        "    val_features = extract_features(autoencoder, x_val)\n",
        "\n",
        "    # Scale the features for the SVM classifier\n",
        "    scaler = StandardScaler()\n",
        "    train_features_scaled = scaler.fit_transform(train_features)\n",
        "    val_features_scaled = scaler.transform(val_features)\n",
        "\n",
        "    # Train an SVM classifier using cross-validation\n",
        "    svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "    scores = cross_val_score(svm_classifier, train_features_scaled, y_train, cv=5)\n",
        "    avg_accuracy = np.mean(scores)\n",
        "\n",
        "    # Store the results\n",
        "    cv_results[(bottleneck_size, batch_size, patience)] = avg_accuracy\n",
        "    print(f\"Bottleneck size {bottleneck_size}, Batch size {batch_size}, Patience {patience} - CV Accuracy: {avg_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "W7LbBWHDq2l0",
        "outputId": "243480f7-a8ac-4265-a582-a1c10d58b3c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SAE with bottleneck size 32, batch size 64, patience 5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'build_sae' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-94d212854a18>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Build the autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_sae' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "E5LzajZxrAGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}